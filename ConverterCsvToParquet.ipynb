{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T01:52:30.750539Z",
     "start_time": "2021-06-15T01:52:30.740587Z"
    }
   },
   "outputs": [],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T01:59:06.701824Z",
     "start_time": "2021-06-16T01:58:26.319305Z"
    }
   },
   "outputs": [],
   "source": [
    "import py7zr\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pc\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from ftplib import FTP\n",
    "\n",
    "ftp = FTP('ftp.mtps.gov.br')\n",
    "ftp.login()\n",
    "ftp.cwd('pdet/microdados/CAGED/')\n",
    "dir_list = []\n",
    "dir_list = ftp.nlst()[:]\n",
    "#bike_share_file = 'CAGEDEST_012007.txt'\n",
    "#parquet_file = 'CAGEDEST_012007.parquet'\n",
    "#arrow_table = pc.read_csv(bike_share_file, parse_options=pc.ParseOptions(delimiter=\";\"),read_options=pc.ReadOptions(encoding='cp1252'))\n",
    "#pq.write_table(arrow_table, parquet_file)\n",
    "start_Geral = datetime.now()\n",
    "for dir in dir_list:\n",
    "    names_files = glob.glob(\"*.parquet\")[:]\n",
    "    try:\n",
    "        ftp.cwd(dir)\n",
    "    except:\n",
    "        continue\n",
    "    file_list=[]\n",
    "    file_list = ftp.nlst()[:]\n",
    "    for file in file_list:\n",
    "        if any(os.path.splitext(file)[0]+\".parquet\" in n for n in names_files):\n",
    "            continue\n",
    "        if(os.path.splitext(file)[1]=='.7z'):\n",
    "            try:\n",
    "                start = datetime.now()  \n",
    "                name_file=file[:-3]\n",
    "                with open(file, 'wb') as fp:\n",
    "                    ftp.retrbinary('RETR '+file, fp.write)\n",
    "                with py7zr.SevenZipFile(file, mode='r') as z:\n",
    "                    z.extractall()\n",
    "                with py7zr.SevenZipFile(file, 'r') as zip:\n",
    "                     allfiles = zip.getnames()\n",
    "                share_file = allfiles[0]\n",
    "                parquet_file = name_file[-4:]+name_file[-6:-4]+'.parquet'\n",
    "                arrow_table = pc.read_csv(share_file, parse_options=pc.ParseOptions(delimiter=\";\"),read_options=pc.ReadOptions(encoding='cp1252'))\n",
    "                pq.write_table(arrow_table, parquet_file)\n",
    "                os.remove(name_file+'.txt')\n",
    "                os.remove(name_file+'.7z')\n",
    "                end = datetime.now()\n",
    "                x = end - start\n",
    "                print('O arquivo: ',name_file)\n",
    "                print(f'Write process took {x} seconds.')\n",
    "            except:\n",
    "                continue\n",
    "    ftp.cwd('..')\n",
    "end = datetime.now()\n",
    "x = end - start_Geral\n",
    "print(f'Tempo Total {x} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T01:58:04.451207Z",
     "start_time": "2021-06-16T01:58:04.446202Z"
    }
   },
   "outputs": [],
   "source": [
    "name_file\n",
    "name_file[-4:]\n",
    "name_file[-6:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T09:44:07.121327Z",
     "start_time": "2021-06-16T09:44:05.560993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 453984576 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-48285025d315>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnames_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\\parquet\\*.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dtypes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dtypes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \"\"\"\n\u001b[0;32m    458\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m     return impl.read(\n\u001b[0m\u001b[0;32m    460\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m         )\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             return self.api.parquet.read_table(\n\u001b[0m\u001b[0;32m    222\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             ).to_pandas(**to_pandas_kwargs)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pyarrow\\array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[1;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m     \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext_columns_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[1;34m(options, block_table, categories, extension_columns)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;31m# Convert an arrow table to Block from the internal pandas API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1128\u001b[1;33m     result = pa.lib.table_to_blocks(options, block_table, categories,\n\u001b[0m\u001b[0;32m   1129\u001b[0m                                     list(extension_columns.keys()))\n\u001b[0;32m   1130\u001b[0m     return [_reconstruct_block(item, columns, extension_columns)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.table_to_blocks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsdegree\\lib\\site-packages\\pyarrow\\error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 453984576 failed"
     ]
    }
   ],
   "source": [
    "## Gerar arquivo com os datatypes da colunas. \n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pyarrow as pa\n",
    "lprimeiro = True\n",
    "names_files = glob.glob(\".\\parquet\\*.parquet\")[:]\n",
    "for file in names_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    df2 = df.dtypes.to_frame('dtypes').reset_index()\n",
    "    print(type(df2['dtypes']))\n",
    "    df2['dtypes'] = df2['dtypes'].apply(str)\n",
    "    print(df2.head())\n",
    "    df2 = df2.transpose()\n",
    "    df2 = df2.reindex(sorted(df2.columns), axis=1)\n",
    "    df2 = df2.rename(index={0: os.path.splitext(file)[0]})\n",
    "    print(os.path.splitext(file)[0])\n",
    "    if(lprimeiro):\n",
    "        #dffinal=df2\n",
    "        table=pa.Table.from_pandas(df)\n",
    "        lprimeiro=False\n",
    "        del df\n",
    "        del df2\n",
    "    else:\n",
    "        new_table=pa.Table.from_pandas(df)\n",
    "        updated_table = pa.concat_tables([table, new_table], promote=True)\n",
    "        #dffinal= dffinal.append(df2)\n",
    "        #del df\n",
    "        del df2\n",
    "    #parquet_file = name_file[-4:]+name_file[-6:-4]+'.parquet'\n",
    "    #os.rename(r'C:\\Users\\Ron\\Desktop\\Test\\Products.txt',r'C:\\Users\\Ron\\Desktop\\Test\\Shipped Products.txt')\n",
    "dffinal.to_csv('dtypes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T01:51:19.974607Z",
     "start_time": "2021-06-16T01:51:19.958652Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in (dffinal.columns.to_list()):\n",
    "    uniqueValues = dffinal[column].unique()\n",
    "    if(len(uniqueValues)>1):\n",
    "        print(\"Colunas com mais de um conte√∫do: \",column)\n",
    "        print(uniqueValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T01:07:07.333044Z",
     "start_time": "2021-06-15T01:07:02.806090Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T01:48:23.588405Z",
     "start_time": "2021-06-15T01:48:17.749554Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pc\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "bike_share_file = 'CAGEDEST_012007.txt'\n",
    "parquet_file = 'CAGEDEST_012007.parquet'\n",
    "arrow_table = pc.read_csv(bike_share_file, parse_options=pc.ParseOptions(delimiter=\";\"),read_options=pc.ReadOptions(encoding='cp1252'))\n",
    "pq.write_table(arrow_table, parquet_file)\n",
    "\n",
    "end = datetime.now()\n",
    "x = end - start\n",
    "print(f'Write process took {x} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T01:48:28.088792Z",
     "start_time": "2021-06-15T01:48:26.494386Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('CAGEDEST_012007.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T01:48:33.225047Z",
     "start_time": "2021-06-15T01:48:31.397621Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T01:23:19.561056Z",
     "start_time": "2021-06-15T01:23:19.101287Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"CSV2Parquet\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "    \n",
    "    schema = StructType([\n",
    "            StructField(\"col1\", IntegerType(), True),\n",
    "            StructField(\"col2\", IntegerType(), True),\n",
    "            StructField(\"col3\", StringType(), True),\n",
    "            StructField(\"col4\", StringType(), True),\n",
    "            StructField(\"col5\", StringType(), True),\n",
    "            StructField(\"col6\", DoubleType(), True)])\n",
    "    \n",
    "    rdd = sc.textFile(\"CAGEDEST_012007.txt\").map(lambda line: line.split(\";\"))\n",
    "    df = sqlContext.createDataFrame(rdd, schema)\n",
    "    df.write.parquet('input-parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
